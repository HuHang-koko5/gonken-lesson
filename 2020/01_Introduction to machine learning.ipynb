{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to an Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement of purpose\n",
    "\n",
    "This course was made to give non-technical knowledge of a sequence of topics we deem necesary for the conceptual understanding of deep neural networks. While some explanations may seem overly simplistic, we chose to focus on understandability over depth. Topics were chosen from the first two chapters of Ian Goodfellow's book [Deep Learning](https://www.deeplearningbook.org/), and reorganized to fit the narrative we wished to convey. \n",
    "\n",
    "This series of lessons were made by Inoue Sho, Jiawei Li and Nakano Lucas for the Gonsalves lab's 2020 April batch of research students.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intended usage\n",
    "\n",
    "Each section of this course is based on portions of the book, which is available online at https://www.deeplearningbook.org/. These chapters are listed in the beginning of each lesson, and we strongly recommend reading them if you wish to deeply learn the topics we present. \n",
    "\n",
    "This course is superficial by necessity, and many portions of the lessons are supplemented by [external links](https://www.computerhope.com/jargon/e/external_link.htm). We suggest visiting these whenever a topic puzzles you or if something sparks your interest. \n",
    "\n",
    "Made as a series of [Jupyter Notebooks](https://jupyter.org/), this course is intended to be interactive. We encourage you to modify the code presented here and play with them until you understand their specific function. Be sure to make use of [Jupyter's question mark notation](https://jakevdp.github.io/PythonDataScienceHandbook/01.01-help-and-documentation.html). Take this as an opportunity to both learn about not only machine learning, but also Python coding!\n",
    "\n",
    "If you find yourself missing any libraries necessary for a lesson, you can install them right in the notebook using pip (e.g. \"pip install jupyterlab\"). Before going any further, we recommend installing the following libraries:\n",
    "- pandas\n",
    "- scipy\n",
    "- seaborn\n",
    "- sklearn\n",
    "Optional, but recommended:\n",
    "- jupyterlab\n",
    "\n",
    "If you have any questions or feedback, whether about the content or about the form, please contact us at l-nakano-0q2@eagle.sophia.ac.jp.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This course is separated into 11 lessons, and 5 practices, though with the interactive nature of notebooks, we hope you will be attempting each topic on your own as we go along.\n",
    "\n",
    "To fully understand the topics presented, you will need prior knowledge of introductory [statistics](https://www.deeplearningbook.org/contents/prob.html), [linear algebra](https://www.deeplearningbook.org/contents/linear_algebra.html), and python programming.\n",
    "\n",
    "1. Introduction \n",
    "2. Learning Algorithms\n",
    "    - 5.1 Learning Algorithms\n",
    "3. How to Evaluate Your Model\n",
    "    - 11.1 Performance Metrics\n",
    "4. Overfitting and Underfitting\n",
    "    - 5.2 Capacity, Overfitting and Underfitting\n",
    "5. **Practice**: Overfitting and Underfitting\n",
    "6. Bias and Variance / Hyperparameter and Validation Set\n",
    "    - 5.3 Hyperparameters and Validation Sets\n",
    "    - 5.4.4 Trading oï¬€ Bias and Variance to Minimize Mean Squared Error\n",
    "7. Maximum Likelihood Estimation\n",
    "    - 5.5 Maximum Likelihood Estimation\n",
    "8. **Practice**: Unsupervised and Supervised learning\n",
    "9. Introduction of Deeplearning\n",
    "    - Chapter 1\n",
    "    - Introduction to section II\n",
    "    - Introduction to chapter 6\n",
    "10. Gradient-Based Learning and SGD\n",
    "    - 4.3 Gradient-Based Optimization\n",
    "    - 5.9 Stochastic Gradient Descent\n",
    "11. Backpropagation\n",
    "    - 6.3.1 Rectified Linear Units and Their Generalizations\n",
    "12. **Practice**: Neural Network Instruction with PyTorch\n",
    "13. Regularization 1\n",
    "    - 7.1 Parameter Norm Pelalties\n",
    "    - 7.4 Dataset Augmentation\n",
    "    - 7.5 Noise Robustness\n",
    "    - 7.8 Early Stopping\n",
    "14. Regularization 2\n",
    "    - 8.1.3 Batch and Minibatch Algorithms\n",
    "    - 8.7.1 Batch Normalization\n",
    "15. **Practice**: Overfitting and Underfitting \n",
    "16. **Practice**: Regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
